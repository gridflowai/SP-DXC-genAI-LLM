{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee0bc272-05dd-4c92-87ac-178a81ed0f17",
   "metadata": {},
   "source": [
    "------------------------\n",
    "#### Overview on NLTK\n",
    "\n",
    "- Natural language tool kit\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d91ea277-2d6b-445f-ace0-27b513c3b78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b45e2c00-deea-4593-8114-ccd7682ed245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5230571f-02eb-41c9-9fae-4547ca7a23f5",
   "metadata": {},
   "source": [
    "#### Token\n",
    "- A token is a piece of a whole, so a word is a token in a sentence, and a sentence is a token in a paragraph.\n",
    "- Tokenization is the process of splitting a string into a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de96500b-260a-4cef-a2fa-3c3655abd123",
   "metadata": {},
   "outputs": [],
   "source": [
    "mystring = \"My favorite color is blue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f3f81cd-675d-437a-9fba-6aedd4923324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My', 'favorite', 'color', 'is', 'blue']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystring.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ac86601-c637-42d2-8ab7-fd8be8dc1645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My', 'favorite', 'colors', 'are', 'blue,', 'red,', 'and', 'green.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystring = \"My favorite colors are blue, red, and green.\"\n",
    "mystring.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3591747c-2ad3-4c1b-8e3c-c04f2332c0f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://t.co/9z2J3P33Uc']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystring = 'https://t.co/9z2J3P33Uc'\n",
    "mystring.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f7164bf-0d4f-453c-8574-aa8a9f5f475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_list = ['https://t.co/9z2J3P33Uc',\n",
    "               'laugh/cry',\n",
    "               'ðŸ˜¬ðŸ˜­ðŸ˜“ðŸ¤¢ðŸ™„ðŸ˜±',\n",
    "               \"world's problems\",\n",
    "               \"@datageneral\",\n",
    "                \"It's interesting\",\n",
    "               \"don't spell my name right\",\n",
    "               'all-nighter',\n",
    "                \"My favorite color is blue\",\n",
    "                \"My favorite colors are blue, red, and green.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018aea94-a405-4010-9ef0-fdd54d011bd5",
   "metadata": {},
   "source": [
    "#### Method 1 : using word tokenizer of NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3db01499-ec43-4095-9712-db29ed5b210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62d0b8b1-57b4-4a8f-8f22-220a022f1e5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'favorite',\n",
       " 'colors',\n",
       " 'are',\n",
       " 'blue',\n",
       " ',',\n",
       " 'red',\n",
       " ',',\n",
       " 'and',\n",
       " 'green',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize('My favorite colors are blue, red, and green.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2209ad29-6b99-48e0-8b2d-6b0aa553e0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = []\n",
    "\n",
    "for sent in compare_list:\n",
    "    sent_tokens = word_tokenize(sent)\n",
    "\n",
    "    word_tokens.append(sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34d36a74-2536-4fd9-bdc3-00a692c3b43c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['https', ':', '//t.co/9z2J3P33Uc'],\n",
       " ['laugh/cry'],\n",
       " ['ðŸ˜¬ðŸ˜­ðŸ˜“ðŸ¤¢ðŸ™„ðŸ˜±'],\n",
       " ['world', \"'s\", 'problems'],\n",
       " ['@', 'datageneral'],\n",
       " ['It', \"'s\", 'interesting'],\n",
       " ['do', \"n't\", 'spell', 'my', 'name', 'right'],\n",
       " ['all-nighter'],\n",
       " ['My', 'favorite', 'color', 'is', 'blue'],\n",
       " ['My',\n",
       "  'favorite',\n",
       "  'colors',\n",
       "  'are',\n",
       "  'blue',\n",
       "  ',',\n",
       "  'red',\n",
       "  ',',\n",
       "  'and',\n",
       "  'green',\n",
       "  '.']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9268fd-34cc-4b53-95f5-203794f1807f",
   "metadata": {},
   "source": [
    "#### Method 2 - Word Punct tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84e71acb-7eb0-433e-b972-ea0c75a6acc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19be4ca3-3a27-4bef-95a0-f1262aa72b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_tokenizer = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f751f7e-c91d-4586-b020-bcf2289d2433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['https', '://', 't', '.', 'co', '/', '9z2J3P33Uc'],\n",
       " ['laugh', '/', 'cry'],\n",
       " ['ðŸ˜¬ðŸ˜­ðŸ˜“ðŸ¤¢ðŸ™„ðŸ˜±'],\n",
       " ['world', \"'\", 's', 'problems'],\n",
       " ['@', 'datageneral'],\n",
       " ['It', \"'\", 's', 'interesting'],\n",
       " ['don', \"'\", 't', 'spell', 'my', 'name', 'right'],\n",
       " ['all', '-', 'nighter'],\n",
       " ['My', 'favorite', 'color', 'is', 'blue'],\n",
       " ['My',\n",
       "  'favorite',\n",
       "  'colors',\n",
       "  'are',\n",
       "  'blue',\n",
       "  ',',\n",
       "  'red',\n",
       "  ',',\n",
       "  'and',\n",
       "  'green',\n",
       "  '.']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punct_tokens = []\n",
    "\n",
    "for sent in compare_list:\n",
    "    \n",
    "    punct_tokens.append(punct_tokenizer.tokenize(sent))\n",
    "punct_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3a92b3-9a7f-433b-b8f2-27e8551cc97c",
   "metadata": {},
   "source": [
    "#### Method 3 - Tweet tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f69a9322-4749-4c22-a6c0-3f11586fe0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee7f4b05-5b06-4038-b3fe-7c51f27a72db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54cd334c-e255-4a17-892b-30abf7a38a05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://t.co/9z2J3P33Uc']\n",
      "['laugh', '/', 'cry']\n",
      "['ðŸ˜¬', 'ðŸ˜­', 'ðŸ˜“', 'ðŸ¤¢', 'ðŸ™„', 'ðŸ˜±']\n",
      "[\"world's\", 'problems']\n",
      "['@datageneral']\n",
      "[\"It's\", 'interesting']\n",
      "[\"don't\", 'spell', 'my', 'name', 'right']\n",
      "['all-nighter']\n",
      "['My', 'favorite', 'color', 'is', 'blue']\n",
      "['My', 'favorite', 'colors', 'are', 'blue', ',', 'red', ',', 'and', 'green', '.']\n"
     ]
    }
   ],
   "source": [
    "tweet_tokens = []\n",
    "\n",
    "for sent in compare_list:\n",
    "    \n",
    "    print(tweet_tokenizer.tokenize(sent))\n",
    "    \n",
    "    tweet_tokens.append(tweet_tokenizer.tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac218a90-06bc-48a8-a621-105177a3e523",
   "metadata": {},
   "source": [
    "#### Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef34df4d-a639-43ed-bcac-b91350153ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f001264-0084-49f4-82f9-232d9cd606b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'t', 'against', 'under', 'haven', 'couldn', 'will', 'o', \"haven't\", \"wouldn't\", 'is', 'be', \"you'll\", 'that', 'do', 'into', 'any', 'll', 'itself', 'mightn', 'are', 'theirs', 'when', 'so', 's', 'yours', 'doing', 'before', 'own', 'am', \"hadn't\", \"won't\", 'me', \"she's\", \"you're\", \"shan't\", 'y', 'no', 'as', 'with', 'his', 'can', 'this', 'those', \"didn't\", 'hadn', \"isn't\", 'most', 'shouldn', 'and', 'these', 'very', 'don', 'at', 've', 'been', 'both', 'other', 'm', \"wasn't\", 'its', \"doesn't\", 'hers', 'won', 'yourself', 'the', 'ourselves', 'her', 'needn', 'were', 'for', 'what', 'same', 'wasn', 'few', 'ours', 'ain', 'there', 'below', 'some', 'from', 'between', 'whom', 'then', 'about', 're', 'while', 'a', \"hasn't\", 'my', 'wouldn', 'because', 'off', \"needn't\", 'to', 'such', 'have', 'didn', \"mustn't\", \"weren't\", 'or', 'of', 'in', 'having', 'up', 'themselves', 'on', 'you', 'ma', \"should've\", \"you've\", 'it', 'being', 'which', 'over', 'shan', \"it's\", 'them', 'should', 'doesn', 'after', 'aren', \"shouldn't\", \"couldn't\", 'we', 'an', 'our', \"you'd\", 'does', 'each', 'who', 'further', \"don't\", 'again', 'she', 'not', 'they', 'out', 'was', 'has', 'where', 'here', 'herself', \"aren't\", 'if', 'more', 'their', 'your', 'him', 'once', 'had', 'd', 'until', 'isn', 'through', 'nor', 'by', 'did', 'why', 'all', 'only', \"mightn't\", 'hasn', \"that'll\", 'how', 'weren', 'i', 'yourselves', 'down', 'too', 'just', 'during', 'above', 'he', 'than', 'himself', 'but', 'mustn', 'myself', 'now'}\n"
     ]
    }
   ],
   "source": [
    "print(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5ec2a4bc-7575-409a-b6c3-ab5c6a2559e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "98e8d739-6456-4b21-adae-f27b5599579b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = word_tokenize(example_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "22cad6e6-d1af-405e-b98f-70916bb8e566",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dffd3b22-d794-4042-bc7a-01d5545225eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5d91e196-2a42-42f8-90c1-2b40ff100d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sentence = [w for w in word_tokens if not w in list(stop_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8a20faf4-5db7-450c-88d6-c68318932e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'sample',\n",
       " 'sentence',\n",
       " ',',\n",
       " 'showing',\n",
       " 'stop',\n",
       " 'words',\n",
       " 'filtration',\n",
       " '.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662d472a-db12-43ae-93b0-2bd79e756c41",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f36d2963-6c9b-4123-ab6c-1983f04430e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f72e54ef-1189-4d7e-83ca-e2170c0a5f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter   = PorterStemmer()\n",
    "lancaster= LancasterStemmer()\n",
    "sno      = nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "13f22c2b-53f2-4f35-a4d2-d48444de6c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                 Porter Stemmer       lancaster Stemmer    Snowball Stemmer    \n",
      "run                  run                  run                  run                 \n",
      "ran                  ran                  ran                  ran                 \n",
      "runner               runner               run                  runner              \n",
      "running              run                  run                  run                 \n"
     ]
    }
   ],
   "source": [
    "word_list = [\"run\", \"ran\", \"runner\", \"running\"]\n",
    "\n",
    "print(\"{0:20} {1:20} {2:20} {3:20}\".format(\"Word\",\"Porter Stemmer\", \"lancaster Stemmer\", \"Snowball Stemmer\"))\n",
    "\n",
    "for word in word_list:\n",
    "    print(\"{0:20} {1:20} {2:20} {3:20}\".format(word, porter.stem(word), lancaster.stem(word), sno.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e1af7ed1-1652-4aad-a879-330d26587bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                 Porter Stemmer       lancaster Stemmer    Snowball Stemmer    \n",
      "cats                 cat                  cat                  cat                 \n",
      "trouble              troubl               troubl               troubl              \n",
      "troubling            troubl               troubl               troubl              \n",
      "troubled             troubl               troubl               troubl              \n",
      "troublesome          troublesom           troublesom           troublesom          \n"
     ]
    }
   ],
   "source": [
    "word_list = [\"cats\", \"trouble\", \"troubling\", \"troubled\", \"troublesome\"]\n",
    "\n",
    "print(\"{0:20} {1:20} {2:20} {3:20}\".format(\"Word\",\"Porter Stemmer\", \"lancaster Stemmer\", \"Snowball Stemmer\"))\n",
    "\n",
    "for word in word_list:\n",
    "    print(\"{0:20} {1:20} {2:20} {3:20}\".format(word, porter.stem(word), lancaster.stem(word), sno.stem(word))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a32b5f0a-75e6-4a4f-8d38-bd8cce738412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                 Porter Stemmer       lancaster Stemmer    Snowball Stemmer    \n",
      "friend               friend               friend               friend              \n",
      "friendship           friendship           friend               friendship          \n",
      "friends              friend               friend               friend              \n",
      "friendships          friendship           friend               friendship          \n",
      "stabil               stabil               stabl                stabil              \n",
      "destabilize          destabil             dest                 destabil            \n",
      "misunderstanding     misunderstand        misunderstand        misunderstand       \n",
      "railroad             railroad             railroad             railroad            \n",
      "moonlight            moonlight            moonlight            moonlight           \n",
      "football             footbal              footbal              footbal             \n"
     ]
    }
   ],
   "source": [
    "#A list of words to be stemmed\n",
    "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabil\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\"football\"]\n",
    "\n",
    "print(\"{0:20} {1:20} {2:20} {3:20}\".format(\"Word\",\"Porter Stemmer\", \"lancaster Stemmer\", \"Snowball Stemmer\"))\n",
    "for word in word_list:\n",
    "    print(\"{0:20} {1:20} {2:20} {3:20}\".format(word, porter.stem(word), lancaster.stem(word), sno.stem(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a0d731-66cf-40ed-8f75-8724d1d40ca8",
   "metadata": {},
   "source": [
    "#### lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "173e3071-ec70-4e88-bb31-e275381f6658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Init the Wordnet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "876340e7-b135-41d1-b31c-6833b2c6970e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                 WordNetLemmatizer   \n",
      "friend               friend               \n",
      "friendship           friendship           \n",
      "friends              friend               \n",
      "friendships          friendship           \n",
      "stabilize            stabilize            \n",
      "destabilize          destabilize          \n",
      "misunderstanding     misunderstanding     \n",
      "railroad             railroad             \n",
      "moonlight            moonlight            \n",
      "football             football             \n"
     ]
    }
   ],
   "source": [
    "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabilize\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\"football\"]\n",
    "\n",
    "print(\"{0:20} {1:20}\".format(\"Word\",\"WordNetLemmatizer\"))\n",
    "\n",
    "for word in word_list:\n",
    "    print(\"{0:20} {1:20} \".format(word, lemmatizer.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d57c7eb-ec13-4e5c-b6af-64a66833c5a4",
   "metadata": {},
   "source": [
    "#### ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5ccacaf1-e82d-4aab-8335-e656aeed34db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eb4692bf-9f91-4b50-a87a-2f1022988c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Data science is a wonderful program, \\\n",
    "Data science is a land of opportunities,data science is about machine learning '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "28148c1c-f566-4153-a7d9-7ed7df385cd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Data', 'science', 'is'),\n",
       " ('science', 'is', 'a'),\n",
       " ('is', 'a', 'wonderful'),\n",
       " ('a', 'wonderful', 'program'),\n",
       " ('wonderful', 'program', ','),\n",
       " ('program', ',', 'Data'),\n",
       " (',', 'Data', 'science'),\n",
       " ('Data', 'science', 'is'),\n",
       " ('science', 'is', 'a'),\n",
       " ('is', 'a', 'land'),\n",
       " ('a', 'land', 'of'),\n",
       " ('land', 'of', 'opportunities'),\n",
       " ('of', 'opportunities', ','),\n",
       " ('opportunities', ',', 'data'),\n",
       " (',', 'data', 'science'),\n",
       " ('data', 'science', 'is'),\n",
       " ('science', 'is', 'about'),\n",
       " ('is', 'about', 'machine'),\n",
       " ('about', 'machine', 'learning')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(nltk.word_tokenize(text), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8816245-f022-4977-aa20-87fd68b70ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199b8e3b-7ccb-4d1a-b7bf-59946a2274df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65f052a-2156-4c22-b9b6-a8abcbe03c04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
