{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bea338e-05c4-4a56-8939-908189702ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain\n",
    "# langchain_openai \n",
    "# langchain_community \n",
    "# langchain-text-splitters \n",
    "# langchain-postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bf18736-ba81-476f-b1d6-e195cc9bf730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai.llms import OpenAI\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3169cb5c-b5b4-4737-bc08-ca0c54db2aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model='gpt-3.5-turbo')\n",
    "model = ChatOpenAI(model='gpt-4o-mini')\n",
    "\n",
    "model = ChatOpenAI(model='gpt-4o-mini',\n",
    "                  max_tokens= 100)\n",
    "\n",
    "#model = ChatOpenAI(model='gpt-3.5-turbo-instruct-0914')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdb754a1-5e80-47f1-bcb1-58f6d92304af",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'The sky is'\n",
    "prompt = '''\n",
    "\n",
    "The sky is . \n",
    "\n",
    "Provide 5 top suitable color options\n",
    "\n",
    "Example\n",
    "the color of chair is orange\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2c81383-1f9c-4b6b-bbbc-f7c0d4d58501",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78bc9de7-6899-4337-a0d0-34cff9578a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The sky is:\\n\\n1. Blue\\n2. Gray\\n3. Pink\\n4. Golden\\n5. Lavender'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71f6dc4b-953d-4593-9a76-adb8d04d1f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The sky is:\\n\\n1. Blue\\n2. Gray\\n3. Pink\\n4. Golden\\n5. Lavender', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 30, 'total_tokens': 53, 'completion_tokens_details': {'audio_tokens': 0, 'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0ba0d124f1', 'finish_reason': 'stop', 'logprobs': None}, id='run-012507cd-d537-4b9d-b46e-591cc1665e5e-0', usage_metadata={'input_tokens': 30, 'output_tokens': 23, 'total_tokens': 53, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bef28e5c-0c1c-4309-a609-a5f4cdb05737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': 'The sky is:\\n\\n1. Blue\\n2. Gray\\n3. Pink\\n4. Golden\\n5. Lavender',\n",
       " 'additional_kwargs': {'refusal': None},\n",
       " 'response_metadata': {'token_usage': {'completion_tokens': 23,\n",
       "   'prompt_tokens': 30,\n",
       "   'total_tokens': 53,\n",
       "   'completion_tokens_details': {'audio_tokens': 0,\n",
       "    'reasoning_tokens': 0,\n",
       "    'accepted_prediction_tokens': 0,\n",
       "    'rejected_prediction_tokens': 0},\n",
       "   'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
       "  'model_name': 'gpt-4o-mini-2024-07-18',\n",
       "  'system_fingerprint': 'fp_0ba0d124f1',\n",
       "  'finish_reason': 'stop',\n",
       "  'logprobs': None},\n",
       " 'type': 'ai',\n",
       " 'name': None,\n",
       " 'id': 'run-012507cd-d537-4b9d-b46e-591cc1665e5e-0',\n",
       " 'example': False,\n",
       " 'tool_calls': [],\n",
       " 'invalid_tool_calls': [],\n",
       " 'usage_metadata': {'input_tokens': 30,\n",
       "  'output_tokens': 23,\n",
       "  'total_tokens': 53,\n",
       "  'input_token_details': {'audio': 0, 'cache_read': 0},\n",
       "  'output_token_details': {'audio': 0, 'reasoning': 0}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f4740c7-d903-464d-96b8-2c76e5147f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05f57446-1bf1-423b-814c-2d146bed6b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'additional_kwargs': {'refusal': None},\n",
      " 'content': 'The sky is:\\n\\n1. Blue\\n2. Gray\\n3. Pink\\n4. Golden\\n5. Lavender',\n",
      " 'example': False,\n",
      " 'id': 'run-012507cd-d537-4b9d-b46e-591cc1665e5e-0',\n",
      " 'invalid_tool_calls': [],\n",
      " 'name': None,\n",
      " 'response_metadata': {'finish_reason': 'stop',\n",
      "                       'logprobs': None,\n",
      "                       'model_name': 'gpt-4o-mini-2024-07-18',\n",
      "                       'system_fingerprint': 'fp_0ba0d124f1',\n",
      "                       'token_usage': {'completion_tokens': 23,\n",
      "                                       'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
      "                                                                     'audio_tokens': 0,\n",
      "                                                                     'reasoning_tokens': 0,\n",
      "                                                                     'rejected_prediction_tokens': 0},\n",
      "                                       'prompt_tokens': 30,\n",
      "                                       'prompt_tokens_details': {'audio_tokens': 0,\n",
      "                                                                 'cached_tokens': 0},\n",
      "                                       'total_tokens': 53}},\n",
      " 'tool_calls': [],\n",
      " 'type': 'ai',\n",
      " 'usage_metadata': {'input_token_details': {'audio': 0, 'cache_read': 0},\n",
      "                    'input_tokens': 30,\n",
      "                    'output_token_details': {'audio': 0, 'reasoning': 0},\n",
      "                    'output_tokens': 23,\n",
      "                    'total_tokens': 53}}\n"
     ]
    }
   ],
   "source": [
    "# Pretty-print the AI message object\n",
    "pprint(dict(completion))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20644a8-7930-4a1f-b997-efbb18e27ba4",
   "metadata": {},
   "source": [
    "#### Example 02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22997d8a-7e5f-46bc-83a2-11295dcfcff8",
   "metadata": {},
   "source": [
    "#### System role\n",
    "Enables the developer to specify instructions the model should use to answer a user question.\n",
    "\n",
    "#### User role\n",
    "The individual asking questions and generating the queries sent to the model.\n",
    "\n",
    "#### Assistant role\n",
    "The model’s responses to the user’s query.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11e5b70-3fba-4523-b1f7-07481d0da11b",
   "metadata": {},
   "source": [
    "langchain way ...\n",
    "\n",
    "#### HumanMessage\n",
    "A message sent from the perspective of the human (user), with the user role.\n",
    "\n",
    "#### AIMessage\n",
    "A message sent from the perspective of the AI the human is interacting with, with the assistant role.\n",
    "\n",
    "#### SystemMessage\n",
    "A message setting the instructions the AI should follow, with the system role.\n",
    "\n",
    "#### ChatMessage\n",
    "A message allowing for arbitrary setting of roles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef3e85ca-db7f-4a0d-9825-136bf93428e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1003833e-1f69-4ff8-a063-ffac46c890d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f826cbe9-dc9f-4956-afd4-999c2691daa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08a486d7-284e-4fc4-ba6b-c2bca0893129",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [HumanMessage('What is the capital of France')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1aaf38f-b707-40a2-9e3a-5780333b4071",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79f87fac-05d6-42d0-adc2-888114b583a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 13, 'total_tokens': 20, 'completion_tokens_details': {'audio_tokens': 0, 'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-bce59100-55c8-4d78-b961-5ca61bbb4a52-0', usage_metadata={'input_tokens': 13, 'output_tokens': 7, 'total_tokens': 20, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82d2d238-f08d-45fe-b39b-6da350132bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "250dead9-cb73-4528-8f33-e16d36a17665",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82829000-98b9-4325-8ace-bcbdf75be17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b046c498-04a9-4197-95b2-55ab90e7c273",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_msg = SystemMessage('You are a helpful assistant')\n",
    "human_msg  = HumanMessage('What is the capital of India')\n",
    "\n",
    "completion = model.invoke([system_msg, human_msg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4c9131f-f95d-46f2-99bd-8054e8e2b2f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of India is New Delhi.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 22, 'total_tokens': 30, 'completion_tokens_details': {'audio_tokens': 0, 'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-039333f2-fd6d-421c-93b0-2d6d8d5ef9b9-0', usage_metadata={'input_tokens': 22, 'output_tokens': 8, 'total_tokens': 30, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2729b079-245d-461c-abb1-38b1d21e3ec8",
   "metadata": {},
   "source": [
    "#### Making LLM prompts reusable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af1e80a9-2321-442b-a9ac-95fa2d801f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19db57bd-4ed5-4218-a8c4-31490bdced0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the reusable prompt template\n",
    "template = \"What is the capital of {country}?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38bd5187-e466-45cd-bd93-5dc1a1796645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PromptTemplate object with a placeholder for 'country'\n",
    "prompt = PromptTemplate(\n",
    "    input_variables= [\"country\"],\n",
    "    template       = template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7b02ac7-e92d-4860-97e2-45fad511b947",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhupe\\AppData\\Local\\Temp\\ipykernel_29248\\2633210295.py:2: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  llm = OpenAI()\n"
     ]
    }
   ],
   "source": [
    "# Initialize the OpenAI model\n",
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a40530c4-b385-45b0-9251-c216e49e4aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the template to fill in different countries\n",
    "query_france  = prompt.format(country=\"France\")\n",
    "query_germany = prompt.format(country=\"Germany\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95ab0641-5ea3-4c70-af9d-0baf5f0d75f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate responses\n",
    "response_france  = llm.invoke(query_france)\n",
    "response_germany = llm.invoke(query_germany)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99571ff1-3bea-4e70-a165-1438ca9e71d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "\n",
      "The capital of Germany is Berlin.\n"
     ]
    }
   ],
   "source": [
    "print(response_france)\n",
    "print(response_germany)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3dd062-58fb-4be1-a354-1181a3e19f5b",
   "metadata": {},
   "source": [
    "#### Example 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb3bfa96-2a7f-4e49-981b-f61f8bb26a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c845325e-3cf2-49d5-bd77-1125356c1a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "726c18fc-ffaa-4375-8f89-77d4aaacaeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reusable HumanMessage with placeholders for 'country' and 'capital'\n",
    "def create_human_message(country, capital):\n",
    "    return HumanMessage(content=f\"Desribe {capital} of the {country}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9b49fa7a-b3ff-46f6-bbb2-f82fec2470a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate HumanMessage for different countries and capitals\n",
    "human_message_france  = create_human_message(country=\"France\", capital=\"Paris\")\n",
    "human_message_germany = create_human_message(country=\"Germany\", capital=\"Berlin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e111ae79-7f06-41e7-a382-158a242dceb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the model with these human messages\n",
    "response_france  = llm.invoke([human_message_france])\n",
    "response_germany = llm.invoke([human_message_germany])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "96f1a2c9-6f67-4c33-b781-dc0b313db1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Paris, the capital city of France, is known for its romantic atmosphere, stunning architecture, and rich history. Situated along the banks of the Seine River, the city is divided into 20 arrondissements (districts), each with its own unique charm and character. The city is home to iconic landmarks such as the Eiffel Tower, Notre-Dame Cathedral, and the Arc de Triomphe, which draw millions of tourists every year.\n",
      "\n",
      "The streets of Paris are lined with beautiful Haussmannian buildings, adorned with ornate balconies and intricate details. The city is also filled with charming cafes, bustling markets, and quaint boutiques, making it a paradise for shoppers and foodies alike.\n",
      "\n",
      "One of the most iconic features of Paris is its wide tree-lined boulevards, which are perfect for leisurely strolls and people-watching. The city is also home to numerous parks and gardens, such as the Jardin du Luxembourg and the Tuileries Garden, providing peaceful oases amidst the bustling city.\n",
      "\n",
      "Paris is also a hub for art and culture, with world-renowned museums such as the Louvre and the Musée d'Orsay showcasing some of the most famous works of art in the world. The city also has a\n"
     ]
    }
   ],
   "source": [
    "print(response_france)  # Output: The capital of France is Paris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0fb23017-59da-4c6f-a2a3-64281545af87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Berlin is the capital and largest city of Germany, located in the northeastern part of the country. It is known for its vibrant culture, diverse history, and thriving arts scene.\n",
      "\n",
      "The city is home to many iconic landmarks, including the Brandenburg Gate, the Reichstag building, and the Berlin Wall, which was once a symbol of the city's division during the Cold War. Today, remnants of the wall can still be found throughout the city, serving as a reminder of its tumultuous past.\n",
      "\n",
      "Berlin is also known for its many museums and galleries, including the famous Museum Island, which is home to five world-renowned museums. The city is also a hub for contemporary art, with numerous galleries and street art adorning its walls.\n",
      "\n",
      "The city has a lively atmosphere, with bustling streets, numerous cafes and restaurants, and a thriving nightlife. It has a diverse population, with people from all over the world calling it home.\n",
      "\n",
      "Despite its modern and cosmopolitan feel, Berlin has a rich history that can be seen in its architecture and monuments. From the grand buildings of the Prussian era to the modern high-rises, Berlin's skyline is a mix of old and new.\n",
      "\n",
      "Overall, Berlin is a dynamic and ever-evolving city that offers something for everyone, making it a\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "print(response_germany)  # Output: The capital of Germany is Berlin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207a9030-063f-4422-bcdc-0cede6ae9991",
   "metadata": {},
   "source": [
    "#### Example - 04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "491c1196-9473-42d6-8b25-b187bd8bf936",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "359d1d76-498b-461d-8a43-971a14b44eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "        Answer the question based on the context given as input\n",
    "        \n",
    "             Context:  {context} \n",
    "             Question: {question} \n",
    "             Answer: \n",
    "             \n",
    "             \"\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ae60b016-2595-48f9-ba8e-2201f3028870",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenAI() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "adb39165-b983-4780-909b-66eaddd0e262",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = template.invoke({ \n",
    "    \"context\": '''advancements in Natural Language Processing (NLP) are prominently marked by \n",
    "    enhanced language models that deliver more coherent and contextually relevant text generation. \n",
    "    ''' ,\n",
    "    \"question\": \"Which model providers offer LLMs?\"\n",
    " }) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "327b88f2-eee8-469a-84fe-912312c054e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='\\n        Answer the question based on the context given as input\\n        \\n             Context:  advancements in Natural Language Processing (NLP) are prominently marked by \\n    enhanced language models that deliver more coherent and contextually relevant text generation. \\n     \\n             Question: Which model providers offer LLMs? \\n             Answer: \\n             \\n             ')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9bc37495-4420-4697-a667-81aded87d13f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe model providers that offer LLMs are those who are continuously working on advancements in Natural Language Processing (NLP). Some of the prominent providers include OpenAI, Google, Microsoft, and Facebook.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02109b28-b0cd-416e-857a-daca407a0cfc",
   "metadata": {},
   "source": [
    "Another way ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2d24ecb2-e1c8-40cf-bae2-ac250d32e80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template with placeholders\n",
    "template = PromptTemplate(\n",
    "    input_variables= [\"context\", \"question\"],\n",
    "    template       = \"Context: {context}\\nQuestion: {question}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b4239e14-8925-4ccc-b359-627672a6e201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the context and question\n",
    "context = \"The most recent advancements in Natural Language Processing include improvements in transformer architectures, fine-tuning techniques, and the development of more efficient models.\"\n",
    "question = \"Which model providers offer LLMs?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "568d4678-4c24-45e7-9b43-3ea433cef22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the template with the provided context and question\n",
    "formatted_prompt = template.invoke({\n",
    "    \"context\": context,\n",
    "    \"question\": question\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3b587b11-e442-4b48-89e5-02d48654d4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI model\n",
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4f09f9dd-86dd-4c2f-a57a-f08dea40a59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a response using the formatted prompt\n",
    "response = llm.invoke(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "198562c9-3c6b-4045-b0b5-070052d76156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Prompt:\n",
      "text='Context: The most recent advancements in Natural Language Processing include improvements in transformer architectures, fine-tuning techniques, and the development of more efficient models.\\nQuestion: Which model providers offer LLMs?'\n",
      "\n",
      "Response:\n",
      "\n",
      "\n",
      "Some model providers that offer LLMs (Language Model Models) include OpenAI, Google, Microsoft, Hugging Face, and Facebook.\n"
     ]
    }
   ],
   "source": [
    "# Print the formatted prompt and response\n",
    "print(\"Formatted Prompt:\")\n",
    "print(formatted_prompt)\n",
    "print(\"\\nResponse:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753da675-6223-4122-96f8-4225f73ca568",
   "metadata": {},
   "source": [
    "#### chat prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e1460561-3b24-45a6-8ac4-958476a70d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0c19b0ac-02ce-45f4-b6c1-351adbb78147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the chat prompt template\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'Answer the question based on the provided context.'),\n",
    "    ('human', 'Context: {context}'),\n",
    "    ('human', 'Question: {question}'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "57600eef-8581-4970-a4ed-cdf2cd424abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the context and question\n",
    "context  = \"The most recent advancements in Natural Language Processing include improvements in transformer architectures, fine-tuning techniques, and the development of more efficient models.\"\n",
    "question = \"Which model providers offer LLMs?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bbbb868e-1d7b-4402-975d-eaa9a91ac453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the template with the provided context and question\n",
    "formatted_prompt = template.invoke({\n",
    "    \"context\":  context,\n",
    "    \"question\": question\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d01d9da4-9362-4d9d-9833-146164a3bf2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Answer the question based on the provided context.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Context: The most recent advancements in Natural Language Processing include improvements in transformer architectures, fine-tuning techniques, and the development of more efficient models.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Question: Which model providers offer LLMs?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3a68bc85-a9c4-4cc3-abf5-0e4f0eaa39b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming formatted_prompt is your ChatPromptTemplate instance with messages\n",
    "messages_content = \"\\n\".join([msg.content for msg in formatted_prompt.messages])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8eee6f43-b5d9-4f64-b535-ac42199c35cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the question based on the provided context.\n",
      "Context: The most recent advancements in Natural Language Processing include improvements in transformer architectures, fine-tuning techniques, and the development of more efficient models.\n",
      "Question: Which model providers offer LLMs?\n"
     ]
    }
   ],
   "source": [
    "print(messages_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b19f6d78-3d07-4f64-9271-3e801930f8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI model\n",
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "80b19d20-7298-4af8-871a-4dcddcfae60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a response using the messages content\n",
    "response = llm.invoke([SystemMessage(content=messages_content)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "61bebde6-8ccd-4e11-8ac4-9d27d0bb3c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Prompt:\n",
      "Answer the question based on the provided context.\n",
      "Context: The most recent advancements in Natural Language Processing include improvements in transformer architectures, fine-tuning techniques, and the development of more efficient models.\n",
      "Question: Which model providers offer LLMs?\n",
      "\n",
      "Response:\n",
      "\n",
      "Answer: It is not specified in the given context which model providers offer LLMs (Language Model Models). However, some popular LLMs in the field of Natural Language Processing include GPT-3 (Generative Pre-trained Transformer), BERT (Bidirectional Encoder Representations from Transformers), and XLNet (eXtreme Learning Network). These models are developed by companies such as OpenAI, Google, and Microsoft respectively.\n"
     ]
    }
   ],
   "source": [
    "# Print the formatted prompt and response\n",
    "print(\"Formatted Prompt:\")\n",
    "print(messages_content)\n",
    "print(\"\\nResponse:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a870d9-04d0-4474-a6a8-9c563bc92ff8",
   "metadata": {},
   "source": [
    "| **Feature**              | **PromptTemplate**                                         | **ChatPromptTemplate**                                      |\n",
    "|--------------------------|------------------------------------------------------------|-------------------------------------------------------------|\n",
    "| **Use Case**             | Single-turn or one-shot tasks (e.g., questions, completions).| Multi-turn interactions (e.g., conversations, chat history). |\n",
    "| **Structure**            | Single text with placeholders (e.g., `\"{variable}\"`).       | Multiple structured messages (e.g., `SystemMessage`, `HumanMessage`). |\n",
    "| **Message Types**        | Only one generic type of input (text).                     | Supports different message roles: `SystemMessage`, `HumanMessage`, `AIMessage`. |\n",
    "| **Conversation Context** | No inherent context management across interactions.        | Manages multi-turn interactions with contextual messages.    |\n",
    "| **Flexibility**          | Simple, best for straightforward formatting of inputs.     | More flexible for handling complex interactions and roles.   |\n",
    "| **Typical Application**  | Basic tasks (e.g., generating a single prompt or response). | Chatbots, dialogue systems, or any AI that manages conversations. |\n",
    "| **Example**              | `The capital of {country} is {capital}.`                   | `System: \"You are an assistant.\"` <br> `Human: \"What is the capital of {country}?\"` |\n",
    "| **Format**               | Direct text formatting.                                    | Multi-role chat with message objects for system, human, and AI roles. |\n",
    "| **When to Use**          | For simple tasks where single-shot prompting is enough.    | For scenarios that require maintaining chat history or multiple roles. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9617dd77-cd97-4770-948f-5a39c52a8349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a reusable ChatPromptTemplate with system and human messages\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'Answer the question based on the provided context.'),\n",
    "    ('human',  'Context: {context}'),\n",
    "    ('human',  'Question: {question}')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fe1b5ec0-1d1d-473a-adee-e44a30ae8b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the ChatOpenAI model\n",
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "eec5a4b7-5158-498e-8cf4-522aceea4e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the template with specific context and question\n",
    "prompt = template.invoke({\n",
    "    \"context\": \"The most recent advancements in NLP and AI.\",\n",
    "    \"question\": \"Which model providers offer LLMs?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ad93d4bd-41cf-4b14-b1bb-f771bc239529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the completion from the model\n",
    "completion = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "43e6c919-f37d-4220-bcde-2dd732cfa5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some of the model providers that offer large language models (LLMs) include OpenAI with GPT-3, Google with BERT, and Microsoft with Turing. These models represent some of the most recent advancements in natural language processing (NLP) and artificial intelligence (AI).'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e72e76d-e297-46ee-9818-6526e87b3272",
   "metadata": {},
   "source": [
    "## Getting Specific Formats out of  LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2929529e-e3e2-4ec1-8dbb-d6bcba80b871",
   "metadata": {},
   "source": [
    "When generating JSON, the first task is to define the schema you want the LLM to respect when producing the output. \n",
    "\n",
    "Then, you should include that schema in the prompt, along with the text you want to use as the source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "76f27628-c6b0-4f09-9f31-0b9fc1a10947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel  # Importing directly from pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fbf5cbeb-b1e3-47b6-b61c-0e9722dc4544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom structure for the answer and justification\n",
    "class AnswerWithJustification(BaseModel):\n",
    "    '''An answer to the user question along with justification.'''\n",
    "    answer:        str  # The answer to the user's question\n",
    "    justification: str  # Justification for the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fb9d62d8-4d0e-4ff9-99ba-af979afc7468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the ChatOpenAI model\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1c947ee8-be21-44e8-b0c1-4351d5d9e2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable structured output by using the defined BaseModel\n",
    "structured_llm = llm.with_structured_output(AnswerWithJustification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "90e22c64-ca7d-40bf-a93e-1044c3a90010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the model with a query\n",
    "result = structured_llm.invoke(\"What weighs more, a pound of feathers or a pound of bricks?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bd347e02-e9a7-44a4-bcdd-cff0b361c76b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnswerWithJustification(answer='They weigh the same.', justification='Both a pound of feathers and a pound of bricks weigh one pound. The difference lies in the volume and density of the two substances.')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "705ca74f-f2a2-4d70-8fd7-8a1c64f2cad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer='They weigh the same.' justification='Both a pound of feathers and a pound of bricks weigh one pound. The difference lies in the volume and density of the two substances.'\n"
     ]
    }
   ],
   "source": [
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ab80ca5f-e079-4d7d-8337-549c097f65a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'They weigh the same.',\n",
       " 'justification': 'Both a pound of feathers and a pound of bricks weigh one pound. The difference lies in the volume and density of the two substances.'}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21036ced-bd55-48a4-801e-a3f1bc6711e3",
   "metadata": {},
   "source": [
    "**Example 1:** Returning Movie Recommendations with Justifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3feb3837-df27-4e6b-b129-9de4a9297779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a structure for movie recommendations\n",
    "class MovieRecommendation(BaseModel):\n",
    "    movie_title:   str  # The title of the recommended movie\n",
    "    genre:         str  # The genre of the movie\n",
    "    justification: str  # Why this movie was recommended\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "af009668-1589-4d0c-b1f9-ffde5b799ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM model\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c7b1613d-32df-43df-ae8a-dbdfb0942fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable structured output\n",
    "structured_llm = llm.with_structured_output(MovieRecommendation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "637a68c2-484c-4062-8a15-e09923f2f2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke with a prompt asking for movie recommendations\n",
    "prompt = \"Can you recommend a sci-fi movie and explain why it’s a good choice?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9c9e1b24-fd19-49c8-9d19-fd5a1867a3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = structured_llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a5651ef1-17c9-4dfd-9bd0-e302b961910f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie_title='Interstellar' genre='Sci-Fi' justification=\"Interstellar is a highly acclaimed movie directed by Christopher Nolan. It's a great choice for those who love sci-fi movies because it combines mind-bending concepts about space and time with a compelling human story. The film is visually stunning, and it's backed by solid performances from a strong cast, including Matthew McConaughey and Anne Hathaway. Moreover, it tackles complex ideas like black holes, time dilation, and wormholes in a way that's accessible to a general audience. The film also boasts an emotional core, exploring themes of love, sacrifice, and the survival of the human race.\"\n"
     ]
    }
   ],
   "source": [
    "# Print the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6f65fc26-04fd-4b77-9eef-0e0d4b4db948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie_title': 'Interstellar',\n",
       " 'genre': 'Sci-Fi',\n",
       " 'justification': \"Interstellar is a highly acclaimed movie directed by Christopher Nolan. It's a great choice for those who love sci-fi movies because it combines mind-bending concepts about space and time with a compelling human story. The film is visually stunning, and it's backed by solid performances from a strong cast, including Matthew McConaughey and Anne Hathaway. Moreover, it tackles complex ideas like black holes, time dilation, and wormholes in a way that's accessible to a general audience. The film also boasts an emotional core, exploring themes of love, sacrifice, and the survival of the human race.\"}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdad752f-c30b-4335-a91d-30d2cd608874",
   "metadata": {},
   "source": [
    "**Example 2:** Returning Product Reviews with Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a49feae1-fe3b-4382-a2da-74ab083269f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a structure for product review\n",
    "class ProductReview(BaseModel):\n",
    "    product_name: str    # Name of the product being reviewed\n",
    "    score:        float  # Rating out of 5\n",
    "    review:       str    # Review explanation for the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a7ca33cc-5ccd-4240-a50d-d63108c89553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM model\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "de6c9d91-e884-47b7-b7bd-aad8a701c91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable structured output\n",
    "structured_llm = llm.with_structured_output(ProductReview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "56d92192-0346-47cf-9950-35410845a754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the model with a prompt about product reviews\n",
    "prompt = \"Can you review the iPhone 14 and give it a score out of 5?\"\n",
    "result = structured_llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "01e7fba6-b04a-45e4-9641-fafed10ca613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'product_name': 'iPhone 14',\n",
       " 'score': 4.0,\n",
       " 'review': 'The iPhone 14 is a great device with impressive performance and features. It has a sleek design, powerful hardware, and a fantastic camera. The only downside is the high price tag.'}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac73f52-87a8-4065-818e-7dab8a5f6120",
   "metadata": {},
   "source": [
    "**Example 3:** Returning Travel Itinerary with Activity Suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4edc9df1-c018-40c3-9731-682757b4b8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a structure for travel itinerary\n",
    "class TravelItinerary(BaseModel):\n",
    "    destination: str        # Destination city or country\n",
    "    days:        int        # Number of days for the itinerary\n",
    "    activities:  list[str]  # List of suggested activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "62e985c3-e4b7-441d-ab8e-bccb44114859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM model\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ea835a61-23e2-438e-8d28-fd128148f3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable structured output\n",
    "structured_llm = llm.with_structured_output(TravelItinerary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "58f80dba-eb38-4e98-838a-4e548b752719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the model with a travel prompt\n",
    "prompt = \"Plan a 3-day trip to Paris with activity suggestions for each day.\"\n",
    "result = structured_llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "99387236-b33a-4d8e-b905-fbe0cfe6ecb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "destination='Paris' days=3 activities=['Visit the Eiffel Tower', 'Explore the Louvre Museum', 'Stroll through Montmartre', 'Relax at Luxembourg Gardens', 'Take a Seine River cruise', 'Visit Notre-Dame Cathedral', 'Discover the Palace of Versailles']\n"
     ]
    }
   ],
   "source": [
    "# Print the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a0fb7252-98cd-4604-bcbb-ff08bb3a6850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'destination': 'Paris',\n",
       " 'days': 3,\n",
       " 'activities': ['Visit the Eiffel Tower',\n",
       "  'Explore the Louvre Museum',\n",
       "  'Stroll through Montmartre',\n",
       "  'Relax at Luxembourg Gardens',\n",
       "  'Take a Seine River cruise',\n",
       "  'Visit Notre-Dame Cathedral',\n",
       "  'Discover the Palace of Versailles']}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d8b371-c72a-4c64-ac00-492b906b131c",
   "metadata": {},
   "source": [
    "**Example 4:** Structured Weather Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "254fb93e-11d7-4a0b-8283-b6b31fe76fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a structure for weather forecast\n",
    "class WeatherForecast(BaseModel):\n",
    "    city:        str  # Name of the city\n",
    "    date:        str  # Forecast date\n",
    "    temperature: str  # Temperature in Celsius/Fahrenheit\n",
    "    description: str  # Short weather description (e.g., sunny, cloudy, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b6df8822-dd4c-43d9-9715-78cc0209911d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM model\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d5cd153b-334f-4377-815d-880062b6b88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable structured output\n",
    "structured_llm = llm.with_structured_output(WeatherForecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "71443faf-0f94-4cbe-9e7b-444d1c02b4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the model with a weather forecast prompt\n",
    "prompt = \"What will the weather be like in New York tomorrow?\"\n",
    "result = structured_llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "50b717e0-b01f-4460-acd1-4447be2f4390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'city': 'New York',\n",
       " 'date': 'tomorrow',\n",
       " 'temperature': '75°F',\n",
       " 'description': 'sunny'}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b71683-ce30-4ab4-838b-788468dcbdff",
   "metadata": {},
   "source": [
    "**Example 5:** Structured Customer Feedback Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0be57074-2546-4d4b-9529-1cc79922a2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a structure for customer feedback\n",
    "class CustomerFeedback(BaseModel):\n",
    "    feedback_summary:  str  # A summary of the feedback\n",
    "    overall_sentiment: str  # Overall sentiment (Positive, Neutral, Negative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "759a57a0-273c-4276-b17b-a599bc8d569c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM model\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "84e137b8-f68d-4c2b-a690-2300d7ab4880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable structured output\n",
    "structured_llm = llm.with_structured_output(CustomerFeedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5d1f73f8-3354-4b9e-a1dd-310c62ccea42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the model with a customer feedback prompt\n",
    "prompt = \"Summarize the customer feedback for our product launch.\"\n",
    "result = structured_llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "dc36e0e8-8921-4309-9706-bf330790e106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feedback_summary': \"The product launch received a mix of positive and negative feedback. Many customers praised the innovative features and user-friendly design, highlighting the product's effectiveness in solving their problems. However, some users reported issues with durability and functionality, expressing disappointment with the initial performance. Overall, customers appreciated the concept but suggested improvements for future iterations.\",\n",
       " 'overall_sentiment': 'mixed'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39983af1-142d-4bde-9713-826013958301",
   "metadata": {},
   "source": [
    "**Example 5:** Structured Customer Feedback Summary with Sentiment and Confidence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "297f3f8c-6261-4ad7-9411-9ba8a216dfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "983e90ed-0112-4468-adac-02683b234f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a structure for customer feedback with sentiment and confidence score\n",
    "class CustomerFeedback(BaseModel):\n",
    "    feedback_summary:  str  # A summary of the feedback\n",
    "    overall_sentiment: str   = Field(..., pattern =\"^(Positive|Negative|Neutral)$\")  # Ensures the sentiment is one of these values\n",
    "    score:             float = Field(..., gt=0, le=10)                            # Confidence score (float, 1 to 10 scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "16dcce32-495c-4e9c-9c8d-b186f66b1a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM model\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "bf14f734-6ebc-4022-84de-29db670eb8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable structured output with the new model\n",
    "structured_llm = llm.with_structured_output(CustomerFeedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5ac2748c-7770-49e3-afb7-2171307424f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the model with a customer feedback prompt\n",
    "prompt = \"Summarize the customer feedback for our product launch and provide a sentiment with a confidence score.\"\n",
    "result = structured_llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8720fa53-e5b2-4e11-9591-9d6ad40af7e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feedback_summary': 'The product launch was well-received, with many customers praising its innovative features and user-friendly design. However, some users reported issues with the initial setup and customer service response times. Overall, the feedback indicates a strong interest in the product, but there are areas for improvement.',\n",
       " 'overall_sentiment': 'Positive',\n",
       " 'score': 8.5}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "55d4c42d-7e23-4180-b5a2-576c025861db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3010652f-5cf6-4f62-826a-1651d9188746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feedback_summary': 'The product launch was well-received, with many '\n",
      "                     'customers praising its innovative features and '\n",
      "                     'user-friendly design. However, some users reported '\n",
      "                     'issues with the initial setup and customer service '\n",
      "                     'response times. Overall, the feedback indicates a strong '\n",
      "                     'interest in the product, but there are areas for '\n",
      "                     'improvement.',\n",
      " 'overall_sentiment': 'Positive',\n",
      " 'score': 8.5}\n"
     ]
    }
   ],
   "source": [
    "pprint(dict(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd2f1e1-b21f-4ca5-b8c9-65a0560f8ce1",
   "metadata": {},
   "source": [
    "... providing review as input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "355716bd-251b-4ab5-8912-91bf8cc6cef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample customer feedback text\n",
    "feedback_text = \"\"\"\n",
    "I recently purchased your new gadget and I couldn't be happier! \n",
    "The performance exceeded my expectations, and the design is sleek. \n",
    "However, I think the battery life could be improved. \n",
    "Overall, I'm very satisfied with my purchase.\n",
    "\"\"\"\n",
    "\n",
    "feedback_text = \"\"\"\n",
    "Earth is flat\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "620e3865-7c70-43c3-8dfd-8357cea920e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt for summarization and sentiment classification\n",
    "prompt = f\"Summarize the following customer feedback and provide a sentiment with a confidence score:\\n\\n{feedback_text}\"\n",
    "result = structured_llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "95d85033-499c-4b4c-9a21-19484a12e33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feedback_summary': 'The customer believes that the Earth is flat.',\n",
      " 'overall_sentiment': 'Negative',\n",
      " 'score': 2.0}\n"
     ]
    }
   ],
   "source": [
    "# Pretty print the result\n",
    "pprint(result.dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152c5a89-f31a-4d2d-8ced-d41bc174fc5e",
   "metadata": {},
   "source": [
    "Further normalization of the sentiment class ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "67f6ed04-bf47-4b1d-ae5c-86eb296e46cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import  validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9c6f3ad4-714f-4d04-85bf-53f2ec20ab87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhupe\\AppData\\Local\\Temp\\ipykernel_8424\\4142650417.py:6: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.9/migration/\n",
      "  @validator('overall_sentiment', pre=True)\n"
     ]
    }
   ],
   "source": [
    "class CustomerFeedback(BaseModel):\n",
    "    feedback_summary: str\n",
    "    overall_sentiment: str = Field(..., pattern=\"^(Positive|Negative|Neutral)$\")\n",
    "    score: float = Field(..., gt=0, le=10)\n",
    "\n",
    "    @validator('overall_sentiment', pre=True)\n",
    "    def normalize_sentiment(cls, value):\n",
    "        if isinstance(value, str):\n",
    "            value_lower = value.lower()\n",
    "            if \"positive\" in value_lower:\n",
    "                return \"Positive\"\n",
    "            elif \"negative\" in value_lower:\n",
    "                return \"Negative\"\n",
    "            else:\n",
    "                return \"Neutral\"\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "bd9ee499-b1f8-4490-951f-fcc7b0464d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample LLM setup and invocation\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.5)\n",
    "structured_llm = llm.with_structured_output(CustomerFeedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "fa0e6226-fac3-42cc-9d06-a6070ee1f8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example feedback text\n",
    "feedback_text = \"I feel it was a mildly positive experience.\"\n",
    "prompt = f\"Summarize the following customer feedback and provide a sentiment with a confidence score:\\n\\n{feedback_text}\"\n",
    "result = structured_llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f1f3ab35-8aac-40ad-bc3e-35f2bd6be5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CustomerFeedback instance to enforce validation and normalization\n",
    "feedback_instance = CustomerFeedback(**result.dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "4ca56e67-a975-49bd-9d7f-87164861b190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomerFeedback(feedback_summary='Mildly positive experience', overall_sentiment='Positive', score=7.0)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedback_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "87f1dc0b-88ce-4c44-ae06-f096605435f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feedback_summary': 'Mildly positive experience',\n",
      " 'overall_sentiment': 'Positive',\n",
      " 'score': 7.0}\n"
     ]
    }
   ],
   "source": [
    "pprint(feedback_instance.dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a60c876-3372-4252-8ff5-03f4eacd0a22",
   "metadata": {},
   "source": [
    "#### getting confidence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "9d7550bf-32ff-4eb3-abe0-354576303f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, field_validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6bde2f05-82d2-44ed-abf9-d14f310f7013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Pydantic model for the output\n",
    "class SentimentAnalysis(BaseModel):\n",
    "    sentiment: str   = Field(..., pattern=\"^(Positive|Negative|Neutral)$\")\n",
    "    score:     float = Field(..., gt=0, le=10, description=\"Confidence score on a scale from 1 to 10.\")\n",
    "\n",
    "    @field_validator('sentiment', mode='before')\n",
    "    def normalize_sentiment(cls, value):\n",
    "        # Normalize different sentiment outputs\n",
    "        if isinstance(value, str):\n",
    "            value_lower = value.lower()\n",
    "            if \"positive\" in value_lower:\n",
    "                return \"Positive\"\n",
    "            elif \"negative\" in value_lower:\n",
    "                return \"Negative\"\n",
    "            else:\n",
    "                return \"Neutral\"\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "feabc633-80c8-42e1-ba7c-ffb48444a634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4af8e7db-48a6-439d-8841-5bd3f8d64693",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_llm = llm.with_structured_output(SentimentAnalysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "5955d633-19a4-4424-8d9c-6242b573013f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example customer feedback text\n",
    "feedback_text = \"The service was excellent, but there was a slight delay.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b35c2cfc-8946-48b5-bf96-4b14ab9f27c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a detailed prompt asking for sentiment and confidence score\n",
    "\n",
    "prompt = (\n",
    "    f\"Analyze the following feedback and provide the sentiment classification \"\n",
    "    f\"('Positive', 'Negative', 'Neutral') along with a confidence score between (1 to 10):\\n\\n\"\n",
    "    f\"{feedback_text}\"\n",
    ")\n",
    "\n",
    "# prompt = (\n",
    "#     f\"Analyze the following feedback and provide the sentiment classification \"\n",
    "#     f\"('Positive', 'Negative', 'Neutral') along with a confidence score between (10 to 100):\\n\\n\"\n",
    "#     f\"{feedback_text}\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b471f718-3884-4438-8a3e-6b5e570708a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the model\n",
    "result = structured_llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a9480256-601e-4d60-bee8-9166355e7a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 5.0, 'sentiment': 'Neutral'}\n"
     ]
    }
   ],
   "source": [
    "# Pretty print the result\n",
    "pprint(result.dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a73d777-f8b0-4af9-83d7-1b35291f76d5",
   "metadata": {},
   "source": [
    "#### Other Machine-Readable Formats with  Output Parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd00f3c-e73d-461a-bcf7-556a7012bb16",
   "metadata": {},
   "source": [
    "##### Using LLMs for Structured Outputs\n",
    "\n",
    "You can leverage a large language model (LLM) or chat model to generate outputs in various formats, including CSV or XML. This is where output parsers become beneficial. \n",
    "\n",
    "##### Functions of Output Parsers\n",
    "\n",
    "Output parsers are classes designed to help structure responses from large language models. They serve two primary purposes:\n",
    "\n",
    "1. **Providing Format Instructions**  \n",
    "   Output parsers can incorporate additional guidelines in the prompt, assisting the LLM in generating text in a specific format that the parser can easily interpret.\n",
    "\n",
    "2. **Validating and Parsing Output**  \n",
    "   The main role of output parsers is to take the textual output from the LLM or chat model and convert it into a more structured format, such as a list or XML. This process may involve eliminating unnecessary information, correcting incomplete output, and validating the parsed values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "7dcc7eda-e435-4d49-9828-6b6e523f1512",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b49d160d-4544-44e5-a976-48fb61cfd385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the output parser\n",
    "parser = CommaSeparatedListOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a45f50b8-43b3-4f45-94bd-3c774f27586e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input string\n",
    "input_string = \"apple, banana, cherry\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "18e23b2b-5d79-41ae-a94b-e0c1779de360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the input string using the output parser\n",
    "items = parser.invoke(input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "87b30057-ce3b-4436-a805-233fda97c49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'banana', 'cherry']\n"
     ]
    }
   ],
   "source": [
    "# Print the parsed items\n",
    "print(items)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea4f2c2-9142-4233-995d-37af093ad53c",
   "metadata": {},
   "source": [
    "Example : extract medical terms from some text ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "438bea9b-f3eb-4e40-9271-ef5b94349576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt asking for medical terms\n",
    "prompt = (\n",
    "    \"Extract the medical terms from the following text and provide them in CSV format:\\n\\n\"\n",
    "    \"The patient was diagnosed with hypertension and prescribed medication for managing high blood pressure. \"\n",
    "    \"The treatment plan includes regular check-ups, a balanced diet, and exercise to improve overall health.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "52196dcc-dd07-4721-a9bc-0a2c4fb3bc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the language model\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "5305482a-a5a7-4bcc-a087-19ee2ff24e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the response from the language model\n",
    "response = llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "732cb016-e206-49c1-bb9a-7b848821bce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='hypertension, medication, high blood pressure, treatment plan, check-ups, balanced diet, exercise, overall health', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 56, 'total_tokens': 79, 'completion_tokens_details': {'audio_tokens': 0, 'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-cacb9c60-099b-4ffa-b019-d4fc300126e4-0', usage_metadata={'input_tokens': 56, 'output_tokens': 23, 'total_tokens': 79, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "50bbec53-e96a-4dd1-b2ca-048d075136d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the output parser\n",
    "parser = CommaSeparatedListOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "23aa6b74-65c3-4a4d-b4ad-94d54c0d77b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the response to extract medical terms\n",
    "medical_terms_csv = parser.invoke(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "acd3ce60-2c5c-458d-8059-427b36447312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hypertension', 'medication', 'high blood pressure', 'treatment plan', 'check-ups', 'balanced diet', 'exercise', 'overall health']\n"
     ]
    }
   ],
   "source": [
    "# Print the parsed medical terms in CSV format\n",
    "print(medical_terms_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf3bc31-0f66-4e47-ad52-d24243f897f8",
   "metadata": {},
   "source": [
    "#### Assembling the Many Pieces of an LLM Application\n",
    "\n",
    "The key components you’ve learned about so far are essential building blocks of the LangChain framework. This leads us to the critical question: \n",
    "\n",
    "- How Do You Combine Them Effectively to Build Your LLM Application?\n",
    "\n",
    "> RUNNABLE INTERFACE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66b9890-a9c8-4be3-9b02-528764dd9f2e",
   "metadata": {},
   "source": [
    "#### Methods for Input Transformation in LangChain\n",
    "\n",
    "1. **invoke**\n",
    "   - Transforms a single input into an output.\n",
    "\n",
    "2. **batch**\n",
    "   - Efficiently transforms multiple inputs into multiple outputs.\n",
    "\n",
    "3. **stream**\n",
    "   - Streams output from a single input as it’s produced.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "599d770f-cd55-484f-801a-acfdddfdf6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f86160fa-ef29-4496-880b-55dce4c39eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "4472bfa2-877f-40e8-b9a3-581d42585737",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = model.invoke('Hi there!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f65e94f1-50c8-4645-a38c-4a42af376ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I realize this is somewhat off-topic however I needed to ask. Does operating a well-established website like yours require a lot of work? I’m completely new to operating a blog however I do write in my journal everyday. I’d like to start a'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a52d05-1c0f-43ec-8f0b-b32cab932c31",
   "metadata": {},
   "source": [
    "**batch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c7a0891c-29b1-4a76-b941-966732c167e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "completions = model.batch(['Hi there!', 'Bye!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "30a5af6b-ae57-4338-9fa1-7c405a628be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" I'm Ryan, a digital nomad and freelance writer based in Asia. I've been living and working remotely for over three years now, and it has been an incredible experience. I've been able to travel to new places, meet interesting people, and learn about different cultures all while earning a living.\\n\\nThere are a few key things that have helped me make the most of my digital nomad lifestyle. Here are my top tips for anyone who is interested in becoming a digital nomad:\\n\\n1. Start with a solid skill set\\n\\nBefore you can become a successful digital nomad, you need to have a skill or set of skills that you can offer remotely. This could be anything from writing and graphic design to web development or social media management. Take some time to assess your strengths and interests and determine what skills you can develop that are in demand in the remote job market.\\n\\n2. Build a strong online presence\\n\\nHaving a strong online presence is crucial for digital nomads. It's important to have a professional website, active social media accounts, and a strong portfolio showcasing your work. This will not only help you attract potential clients, but also establish credibility and build your personal brand.\\n\\n3. Network, network, network\\n\\nNetworking is key for digital nomads. Attend industry\",\n",
       " '\\n\\nGoodbye! Have a great day!']"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "3b5f80e8-be0a-46ac-9897-f2e207f3f9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Hi there!\n",
      "Completion:  I'm Ryan, a digital nomad and freelance writer based in Asia. I've been living and working remotely for over three years now, and it has been an incredible experience. I've been able to travel to new places, meet interesting people, and learn about different cultures all while earning a living.\n",
      "\n",
      "There are a few key things that have helped me make the most of my digital nomad lifestyle. Here are my top tips for anyone who is interested in becoming a digital nomad:\n",
      "\n",
      "1. Start with a solid skill set\n",
      "\n",
      "Before you can become a successful digital nomad, you need to have a skill or set of skills that you can offer remotely. This could be anything from writing and graphic design to web development or social media management. Take some time to assess your strengths and interests and determine what skills you can develop that are in demand in the remote job market.\n",
      "\n",
      "2. Build a strong online presence\n",
      "\n",
      "Having a strong online presence is crucial for digital nomads. It's important to have a professional website, active social media accounts, and a strong portfolio showcasing your work. This will not only help you attract potential clients, but also establish credibility and build your personal brand.\n",
      "\n",
      "3. Network, network, network\n",
      "\n",
      "Networking is key for digital nomads. Attend industry\n",
      "\n",
      "Input: Bye!\n",
      "Completion: \n",
      "\n",
      "Goodbye! Have a great day!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing the completions for each input\n",
    "for input_text, completion in zip(['Hi there!', 'Bye!'], completions):\n",
    "    print(f\"Input: {input_text}\\nCompletion: {completion}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7551214b-f9a2-402a-84db-169f7c73f0e7",
   "metadata": {},
   "source": [
    "`model.batch`: This method takes a list of input strings and processes them in a single call, allowing for efficient handling of multiple requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b9f0e791-cf2f-439f-9951-7701a502ef88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Good\n",
      "bye\n",
      "!\n",
      " Take\n",
      " care\n",
      "!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in model.stream('Bye!'): \n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd9021b-3bbb-4c46-b93a-355867edc262",
   "metadata": {},
   "source": [
    "`model.stream`: This method takes a single input string and produces output tokens incrementally. This is useful for applications where you want to display the output in real time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4c3b7d-05a0-4bf0-8086-13d85511c03b",
   "metadata": {},
   "source": [
    "**Example :** Multiple Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "4c07d48d-6624-4189-9539-863cd3f3c3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ec712f73-15bc-40c7-a516-123154c52cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of questions\n",
    "questions = [\n",
    "    \"What are the symptoms of COVID-19?\",\n",
    "    \"How does photosynthesis work?\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"Explain the theory of relativity.\",\n",
    "    \"What is the process of evolution?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "1693aa3e-4d25-43be-a2b7-13a95492a663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use batch to get answers for all questions\n",
    "answers = model.batch(questions)\n",
    "\n",
    "# Use batch to get answers for all questions, limiting responses to 3 sentences\n",
    "answers = model.batch([f\"Answer in 3 sentences max: '{question}'\" for question in questions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ec3e2002-2195-4b0c-a75f-ca2772c09f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the symptoms of COVID-19?\n",
      "Answer: Symptoms of COVID-19 include fever, cough, and shortness of breath. Other symptoms may include fatigue, body aches, and loss of taste or smell. If you experience any of these symptoms, it is important to get tested and follow public health guidelines.\n",
      "\n",
      "Question: How does photosynthesis work?\n",
      "Answer: Photosynthesis is the process by which plants, algae, and some bacteria convert sunlight into energy. This energy is used to convert carbon dioxide and water into glucose and oxygen. The chlorophyll in the plant's cells captures sunlight, which then triggers a series of chemical reactions that ultimately produce glucose as a source of food for the plant.\n",
      "\n",
      "Question: What is the capital of France?\n",
      "Answer: The capital of France is Paris. It is known for its iconic landmarks such as the Eiffel Tower and Louvre Museum. Paris is also famous for its art, fashion, and cuisine.\n",
      "\n",
      "Question: Explain the theory of relativity.\n",
      "Answer: The theory of relativity, developed by Albert Einstein, states that the laws of physics are the same for all non-accelerating observers and that the speed of light in a vacuum is constant. It also shows that time can be perceived differently depending on the observer's relative motion. The theory has had a profound impact on our understanding of space, time, and gravity.\n",
      "\n",
      "Question: What is the process of evolution?\n",
      "Answer: Evolution is the process by which species change over time through the accumulation of genetic variations. These variations can occur through mutation, genetic drift, gene flow, and natural selection. Ultimately, evolution leads to the adaptation of species to their environments, resulting in the diversity of life on Earth.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print each question with its corresponding answer\n",
    "for question, answer in zip(questions, answers):\n",
    "    print(f\"Question: {question}\\nAnswer: {(answer.content)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9933041-6253-4801-916e-078d7fc99efa",
   "metadata": {},
   "source": [
    "#### Benefits of Using `batch()`:\n",
    "\n",
    "- **Efficiency**: It processes multiple requests in parallel, reducing overall latency and improving response times.\n",
    "\n",
    "- **Single API Call**: Instead of making multiple sequential API calls, `batch()` combines all inputs into one API call, saving time and resources.\n",
    "\n",
    "- **Simplified Code**: By handling all inputs and outputs at once, `batch()` minimizes the need for writing multiple API requests, leading to cleaner and more maintainable code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87b955c-99c4-41eb-a0f0-64ba06ad571f",
   "metadata": {},
   "source": [
    "#### Key Differences Between Loop and batch():\n",
    "- Loop: Would make one API call for each question.\n",
    "- Batch: Sends all questions in a single API call, processes them, and returns all the answers together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7604100c-3f52-49d3-98a6-a3d61b73b6a1",
   "metadata": {},
   "source": [
    "Another example ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "61fcb0ef-a1a3-4b5f-9f38-23f0ec7bd487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of articles (text snippets)\n",
    "articles = [\n",
    "    \"Artificial intelligence (AI) refers to the simulation of human intelligence in machines.\",\n",
    "    \"Machine learning is a subset of AI that involves the use of algorithms to analyze data.\",\n",
    "    \"Deep learning is a type of machine learning that uses neural networks to model complex patterns.\",\n",
    "    \"Natural language processing (NLP) allows computers to understand and respond to human language.\",\n",
    "    \"Reinforcement learning is an area of machine learning concerned with how agents ought to take actions in an environment.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "7b825b10-7773-48d1-b44f-3a00d55de52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use batch to summarize articles with a sentence limit of 3 sentences\n",
    "summaries = model.batch([f\"Summarize the following article in 2 sentences max: '{article}'\" for article in articles])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "aa77e7ee-f4d3-48f0-935b-662302a2ca6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article: Artificial intelligence (AI) refers to the simulation of human intelligence in machines.\n",
      "Summary: Artificial intelligence (AI) is the replication of human intelligence in machines, allowing them to perform tasks that typically require human intelligence, such as visual perception, speech recognition, decision-making, and language translation.\n",
      "\n",
      "Article: Machine learning is a subset of AI that involves the use of algorithms to analyze data.\n",
      "Summary: Machine learning is a type of artificial intelligence that uses algorithms to analyze data. It is a powerful tool for making predictions and decisions based on patterns in the data.\n",
      "\n",
      "Article: Deep learning is a type of machine learning that uses neural networks to model complex patterns.\n",
      "Summary: Deep learning is a form of machine learning that utilizes neural networks to analyze intricate patterns. It is particularly effective in tasks such as image and speech recognition.\n",
      "\n",
      "Article: Natural language processing (NLP) allows computers to understand and respond to human language.\n",
      "Summary: Natural language processing (NLP) enables computers to interpret and generate human language, improving communication between machines and people.\n",
      "\n",
      "Article: Reinforcement learning is an area of machine learning concerned with how agents ought to take actions in an environment.\n",
      "Summary: Reinforcement learning is a subset of machine learning that focuses on how agents should make decisions in an environment. It involves learning through trial and error to maximize rewards received for certain actions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print each article with its corresponding summary\n",
    "for article, summary in zip(articles, summaries):\n",
    "    print(f\"Article: {article}\\nSummary: {summary.content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7273c3-4321-42e3-81a5-b80f3c0857a0",
   "metadata": {},
   "source": [
    "#### combine a chat model and a prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "75f5e9d6-ff9b-4ad0-ac15-df0d008d7ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "7f944018-2ebb-4e0e-85b2-6d7480df4098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template with a system message and a human input\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are a helpful assistant.'),\n",
    "    ('human', '{question}'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "4f16b4e9-f1fe-48a6-944f-de9ceb826090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the chat model\n",
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "07ba21ec-ea81-4d1f-8752-fe3c03dc5c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine them into a function using the @chain decorator\n",
    "@chain\n",
    "def chatbot(values):\n",
    "    # Create the prompt by passing values into the template\n",
    "    prompt = template.invoke(values)\n",
    "    \n",
    "    # Use the chat model to invoke the prompt and return the result\n",
    "    return model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "231b0610-8ba2-41e7-b6c0-5c219a7d117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the chatbot function by passing a question\n",
    "result = chatbot.invoke({\n",
    "    \"question\": \"Which model providers offer LLMs?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "f05d8b4c-409c-4611-83ff-e0f57e886d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Many law schools offer LLM (Master of Laws) programs, including top law schools such as Harvard Law School, Yale Law School, Stanford Law School, Columbia Law School, and NYU School of Law. Additionally, many other law schools around the world also offer LLM programs, so there are numerous options available depending on your preferences and area of interest.'"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the result\n",
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285373a9-cbd8-4164-ae27-a2d377faa9b6",
   "metadata": {},
   "source": [
    "#### Example: Multi-Step Knowledge Retrieval and Question Answering\n",
    "\n",
    "- The `@chain` decorator in LangChain allows us to create complex `pipelines` where different parts of the workflow (e.g., processing input, calling an LLM, post-processing) are handled sequentially in a reusable and composable way.\n",
    "\n",
    "- here we use @chain to build a chatbot that processes user questions, retrieves data from a mock knowledge base, and then passes the combined context and question to the LLM for a detailed answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ce6ea176-4343-4c96-95fb-8b2fef8c81bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "a2d755a4-d66c-4427-bdf0-be149abd0709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define a mock function for retrieving knowledge base content\n",
    "def retrieve_knowledge_base(question):\n",
    "    knowledge_base = {\n",
    "        \"python\": \"Python is a versatile programming language used for web development, data science, and automation.\",\n",
    "        \"ai\":     \"Artificial intelligence (AI) is the simulation of human intelligence in machines designed to think and learn.\",\n",
    "        \"cloud\":  \"Cloud computing provides scalable computing resources over the internet, enabling flexible IT infrastructure.\"\n",
    "    }\n",
    "    return knowledge_base.get(question.lower(), \"No relevant information found in the knowledge base.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "b58897b3-b563-4ddb-9482-36952098fa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the prompt template\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are a helpful assistant with access to a knowledge base.'),\n",
    "    ('system', 'Knowledge base info: {context}'),\n",
    "    ('human',  '{question}')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "58fc96be-b53c-4051-aace-62aa26ccf9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Initialize the chat model\n",
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "f58ac9ec-3387-4cc4-a739-1344790af43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create the chained function\n",
    "@chain\n",
    "def complex_chatbot(values):\n",
    "    # Step 5: Retrieve relevant knowledge from the mock knowledge base\n",
    "    context = retrieve_knowledge_base(values['question'])\n",
    "    \n",
    "    # Step 6: Pass the knowledge and question to the prompt template\n",
    "    prompt = template.invoke({\n",
    "        \"context\":  context,\n",
    "        \"question\": values['question']\n",
    "    })\n",
    "    \n",
    "    # Step 7: Call the model with the complete prompt\n",
    "    return model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "a9491e5b-c43f-4a27-bdef-f91479d3bde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Use the chatbot function\n",
    "result = complex_chatbot.invoke({\n",
    "    \"question\": \"Tell me about Potato\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "ca4bec99-a2e1-48a1-b9cc-958b65c6a0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potatoes are starchy tuberous crops that belong to the nightshade family. They are a widely consumed food crop around the world and are known for their versatility in cooking. Potatoes can be boiled, baked, fried, mashed, or used in various dishes such as soups, stews, and salads. They are a good source of vitamins, minerals, and dietary fiber. There are many different varieties of potatoes, each with its own unique flavor, texture, and ideal cooking method. Potatoes are also used to make popular products like French fries, potato chips, and vodka.\n"
     ]
    }
   ],
   "source": [
    "# Display the result\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0aceab-c114-48e6-8722-31c50e0d4959",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "langchain_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
